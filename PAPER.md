# Research Paper: Distilling Grokking

## Quick Links

ðŸ“„ **Complete Paper:** [`paper/paper.md`](paper/paper.md)

ðŸ“Š **Figures & Results:** [`paper/figures/`](paper/figures/)

ðŸ”§ **Analysis Scripts:** [`paper/scripts/`](paper/scripts/)

ðŸ“– **Paper Guide:** [`paper/README.md`](paper/README.md)

---

## Paper Summary

**Title:** Distilling Grokking: Knowledge Transfer from Grokked Teacher Models

**Author:** Amund Tveit (Atsentia)

**Repository:** [github.com/atsentia/jax-tunix-grokking](https://github.com/atsentia/jax-tunix-grokking)

### Key Findings

1. **All three distillation strategies succeed** with weight decay = 1.0
   - Logit: 99.98% val acc (converges epoch 123)
   - Attention: 99.96% val acc (converges epoch 121)
   - Feature: 100.00% val acc (converges epoch 91) âš¡ **1.4Ã— faster**

2. **Weight decay is critical** for transferring grokking from teacher to student

3. **Mechanistic insights:**
   - High CKA alignment in early layers (0.89-0.92)
   - Students learn more compact representations (19.7-30.7 vs 38.7-41.2 dims)
   - Feature distillation produces lowest-rank solutions

4. **Delayed generalization persists** even with intermediate supervision

### Paper Structure

```
paper/
â”œâ”€â”€ paper.md                    # Complete manuscript (51KB)
â”œâ”€â”€ README.md                   # Detailed paper guide
â”œâ”€â”€ figures/                    # All visualizations
â”‚   â”œâ”€â”€ paper_teacher_grokking.{png,pdf}
â”‚   â”œâ”€â”€ paper_distillation_comparison.{png,pdf}
â”‚   â”œâ”€â”€ paper_mechanistic_analysis.{png,pdf}
â”‚   â”œâ”€â”€ paper_10x_runs_placeholder.{png,pdf}
â”‚   â””â”€â”€ paper_results_table.tex
â”œâ”€â”€ scripts/                    # Reproducibility
â”‚   â”œâ”€â”€ generate_paper_plots.py
â”‚   â”œâ”€â”€ mechanistic_analysis.py
â”‚   â”œâ”€â”€ plot_mechanistic_analysis.py
â”‚   â””â”€â”€ create_placeholder_figure.py
â””â”€â”€ data/
    â””â”€â”€ mechanistic_analysis_results.json
```

### Sections

1. **Abstract** - High-level summary with key findings
2. **Introduction** - Motivation and contributions
3. **Background** - Grokking phenomenon and knowledge distillation
4. **Methods** - Three distillation strategies with detailed explanations
5. **Experiments** - Results from snapshot runs + mechanistic analysis
6. **Discussion** - Why weight decay matters, why feature distillation is faster
7. **Related Work** - Prior work on grokking and distillation
8. **Future Work** - 10Ã— validation study, 4-bit training, recursive distillation
9. **Conclusion** - Summary and implications
10. **Appendices** - Hyperparameters and reproducibility details

### Status

- âœ… Complete paper draft (40+ pages)
- âœ… All figures generated
- âœ… Mechanistic interpretability analysis complete
- ðŸ”„ Large-scale validation (10Ã— runs) - placeholder added (Â§4.4)
- ðŸ”„ W&B integration - planned

---

**Quick Start:** Read [`paper/paper.md`](paper/paper.md) for the full manuscript.
